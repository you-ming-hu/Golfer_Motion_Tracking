{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.core.config import config as Config\n",
    "\n",
    "with Config:\n",
    "    Config.OUTPUT_DIR = ''\n",
    "    Config.LOG_DIR = ''\n",
    "    Config.DATA_DIR = ''\n",
    "    Config.GPUS = '0'\n",
    "    Config.WORKERS = 0\n",
    "    Config.PRINT_FREQ = 20\n",
    "\n",
    "    # Cudnn related params\n",
    "    Config.CUDNN.BENCHMARK = True\n",
    "    Config.CUDNN.DETERMINISTIC = False\n",
    "    Config.CUDNN.ENABLED = True\n",
    "    \n",
    "    # common params for NETWORK\n",
    "    Config.MODEL.NAME = 'pose_resnet'\n",
    "    Config.MODEL.INIT_WEIGHTS = True\n",
    "    Config.MODEL.PRETRAINED = 'pretrained_weight\\pose_resnet_50_384x288.pth.tar'\n",
    "    Config.MODEL.NUM_JOINTS = 17 #17 for coco, 16 for mpii\n",
    "    Config.MODEL.IMAGE_SIZE = [256,256]  # width * height, ex: 192 * 256\n",
    "    Config.MODEL.EXTRA.NUM_LAYERS = 50\n",
    "    Config.MODEL.EXTRA.DECONV_WITH_BIAS = False\n",
    "    Config.MODEL.EXTRA.NUM_DECONV_LAYERS = 3\n",
    "    Config.MODEL.EXTRA.NUM_DECONV_FILTERS = [256, 256, 256]\n",
    "    Config.MODEL.EXTRA.NUM_DECONV_KERNELS = [4, 4, 4]\n",
    "    Config.MODEL.EXTRA.FINAL_CONV_KERNEL = 1\n",
    "    Config.MODEL.EXTRA.TARGET_TYPE = 'gaussian'\n",
    "    Config.MODEL.EXTRA.HEATMAP_SIZE = [64, 64]  # width * height, ex: 24 * 32\n",
    "    Config.MODEL.EXTRA.SIGMA = 2\n",
    "\n",
    "    Config.MODEL.STYLE = 'pytorch' #caffe or pytorch\n",
    "\n",
    "    Config.LOSS.USE_TARGET_WEIGHT = True\n",
    "\n",
    "    # DATASET related params\n",
    "    Config.DATASET.ROOT = r'D:\\Datasets\\coco2017'\n",
    "    Config.DATASET.DATASET = 'coco' # mpii or coco\n",
    "    Config.DATASET.TRAIN_SET = 'train'\n",
    "    Config.DATASET.TEST_SET = 'validation'\n",
    "    Config.DATASET.DATA_FORMAT = 'jpg'\n",
    "    Config.DATASET.HYBRID_JOINTS_TYPE = ''\n",
    "    Config.DATASET.SELECT_DATA = True\n",
    "\n",
    "    # training data augmentation\n",
    "    Config.DATASET.FLIP = True\n",
    "    Config.DATASET.SCALE_FACTOR = 0.25\n",
    "    Config.DATASET.ROT_FACTOR = 30\n",
    "\n",
    "    # train\n",
    "    Config.TRAIN.LR_FACTOR = 0.1\n",
    "    Config.TRAIN.LR_STEP = [90, 110]\n",
    "    Config.TRAIN.LR = 0.001\n",
    "\n",
    "    Config.TRAIN.OPTIMIZER = 'adam'\n",
    "    Config.TRAIN.MOMENTUM = 0.9\n",
    "    Config.TRAIN.WD = 0.0001\n",
    "    Config.TRAIN.NESTEROV = False\n",
    "    Config.TRAIN.GAMMA1 = 0.99\n",
    "    Config.TRAIN.GAMMA2 = 0.0\n",
    "\n",
    "    Config.TRAIN.BEGIN_EPOCH = 0\n",
    "    Config.TRAIN.END_EPOCH = 140\n",
    "\n",
    "    Config.TRAIN.RESUME = False\n",
    "    Config.TRAIN.CHECKPOINT = ''\n",
    "\n",
    "    Config.TRAIN.BATCH_SIZE = 2\n",
    "    Config.TRAIN.SHUFFLE = True\n",
    "\n",
    "    # testing\n",
    "    # size of images for each device\n",
    "    Config.TEST.BATCH_SIZE = 2\n",
    "    # Test Model Epoch\n",
    "    Config.TEST.FLIP_TEST = True\n",
    "    Config.TEST.POST_PROCESS = True\n",
    "    Config.TEST.SHIFT_HEATMAP = True\n",
    "\n",
    "    Config.TEST.USE_GT_BBOX = True\n",
    "    # nms\n",
    "    Config.TEST.OKS_THRE = 0.5\n",
    "    Config.TEST.IN_VIS_THRE = 0.0\n",
    "    Config.TEST.COCO_BBOX_FILE = ''\n",
    "    Config.TEST.BBOX_THRE = 1.0\n",
    "    Config.TEST.MODEL_FILE = ''\n",
    "    Config.TEST.IMAGE_THRE = 0.0\n",
    "    Config.TEST.NMS_THRE = 1.0\n",
    "\n",
    "    # debug\n",
    "    Config.DEBUG.DEBUG = False\n",
    "    Config.DEBUG.SAVE_BATCH_IMAGES_GT = False\n",
    "    Config.DEBUG.SAVE_BATCH_IMAGES_PRED = False\n",
    "    Config.DEBUG.SAVE_HEATMAPS_GT = False\n",
    "    Config.DEBUG.SAVE_HEATMAPS_PRED = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run pose_estimation/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root, image_size):\n",
    "        self.db = list(pathlib.Path(root).rglob('*.jpg'))\n",
    "        self.image_size = image_size #(W,H)\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.db)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_file = self.db[idx].as_posix()\n",
    "        image = cv2.imread(image_file, cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if (image.shape[0] != self.image_size[1]) or (image.shape[1] != self.image_size[0]):\n",
    "            image = cv2.resize(image,self.image_size)\n",
    "        image = self.transform(image)\n",
    "        return image, image_file\n",
    "\n",
    "r=3/4\n",
    "test_dataset = TestDataset(r'D:\\Datasets\\Golf\\IdeasLab\\validation',[int(1280*r),int(720*r)])\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.models.pose_resnet import get_pose_net\n",
    "from collections import OrderedDict\n",
    "\n",
    "model_weight_path = r'D:\\Portfolio\\Golfer_Motion_Tracking\\DL_approach\\human-pose-estimation\\coco\\pose_resnet_50\\model_best.pth.tar'\n",
    "\n",
    "model = get_pose_net(Config, is_train=False)\n",
    "state_dict = OrderedDict()\n",
    "for k,v in torch.load(model_weight_path).items():\n",
    "    state_dict[k.replace('module.','')] = v\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from lib.utils.transforms import flip_back\n",
    "\n",
    "def get_coor(batch_heatmaps):\n",
    "    batch_size = batch_heatmaps.shape[0]\n",
    "    num_joints = batch_heatmaps.shape[1]\n",
    "    width = batch_heatmaps.shape[3]\n",
    "    heatmaps_reshaped = batch_heatmaps.reshape((batch_size, num_joints, -1))\n",
    "    idx = np.argmax(heatmaps_reshaped, 2)\n",
    "    maxvals = np.amax(heatmaps_reshaped, 2)\n",
    "    maxvals = maxvals.reshape((batch_size, num_joints, 1))\n",
    "    idx = idx.reshape((batch_size, num_joints, 1))\n",
    "    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\n",
    "    preds[:, :, 0] = (preds[:, :, 0]) % width\n",
    "    preds[:, :, 1] = np.floor((preds[:, :, 1]) / width)\n",
    "    pred_mask = np.tile(np.greater(maxvals, 0.0), (1, 1, 2))\n",
    "    pred_mask = pred_mask.astype(np.float32)\n",
    "    preds *= pred_mask\n",
    "    \n",
    "    heatmap_height = batch_heatmaps.shape[2]\n",
    "    heatmap_width = batch_heatmaps.shape[3]\n",
    "    for n in range(batch_size):\n",
    "        for p in range(num_joints):\n",
    "            hm = batch_heatmaps[n][p]\n",
    "            px = int(math.floor(preds[n][p][0] + 0.5))\n",
    "            py = int(math.floor(preds[n][p][1] + 0.5))\n",
    "            if 1 < px < heatmap_width-1 and 1 < py < heatmap_height-1:\n",
    "                diff = np.array([hm[py][px+1] - hm[py][px-1],hm[py+1][px]-hm[py-1][px]])\n",
    "                preds[n][p] += np.sign(diff) * .25\n",
    "    \n",
    "    return preds, maxvals.squeeze(axis=-1)\n",
    "\n",
    "\n",
    "def predict(model,images,axi_flip=True):\n",
    "    with torch.no_grad():\n",
    "        if axi_flip:\n",
    "            flip_pairs = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16]]\n",
    "            flip_images = np.flip(images.numpy(), 3)\n",
    "            flip_images = torch.from_numpy(flip_images.copy())\n",
    "            flip_heatmaps = model(flip_images).numpy()\n",
    "            flip_heatmaps = flip_back(flip_heatmaps,flip_pairs)\n",
    "            flip_heatmaps[:, :, :, 1:] = flip_heatmaps[:, :, :, 0:-1]\n",
    "            \n",
    "            heatmaps = model(images).numpy()\n",
    "            heatmaps = (heatmaps + flip_heatmaps)/2\n",
    "        else:\n",
    "            heatmaps = model(images).numpy()\n",
    "        \n",
    "    joints_coor, joints_score = get_coor(heatmaps)\n",
    "    return heatmaps, joints_coor, joints_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coor_transform(coor, heatmaps, source_images):\n",
    "    heatmaps_shape = np.array(heatmaps.shape[2:][::-1])\n",
    "    source_images_shape = np.array([s.shape[:2][::-1] for s in source_images])[:,None,:]\n",
    "    coor = coor/heatmaps_shape * source_images_shape\n",
    "    coor = np.round(coor).astype(int)\n",
    "    return coor\n",
    "\n",
    "def heatmaps_resize(heatmaps, source_images):\n",
    "    return [cv2.resize(h,i.shape[:2][::-1]).transpose([2,0,1]) for h,i in zip(heatmaps.transpose([0,2,3,1]),source_images)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def save_visualize_result(source_path,source_image,joints_coor,heatmaps):\n",
    "    joints_name = {\n",
    "        0: \"nose\",\n",
    "        1: \"left_eye\",\n",
    "        2: \"right_eye\",\n",
    "        3: \"left_ear\",\n",
    "        4: \"right_ear\",\n",
    "        5: \"left_shoulder\",\n",
    "        6: \"right_shoulder\",\n",
    "        7: \"left_elbow\",\n",
    "        8: \"right_elbow\",\n",
    "        9: \"left_wrist\",\n",
    "        10: \"right_wrist\",\n",
    "        11: \"left_hip\",\n",
    "        12: \"right_hip\",\n",
    "        13: \"left_knee\",\n",
    "        14: \"right_knee\",\n",
    "        15: \"left_ankle\",\n",
    "        16: \"right_ankle\"}\n",
    "    image_copy = source_image.copy()\n",
    "    for j_idx, j_coor in enumerate(joints_coor):\n",
    "        cv2.circle(image_copy,j_coor,3,(255,0,0),-1)\n",
    "        cv2.putText(image_copy, joints_name[j_idx], j_coor, cv2.FONT_HERSHEY_DUPLEX, 0.4, (255, 0, 0))\n",
    "    fig = plt.figure(figsize=(128,72),dpi=15)\n",
    "    plt.imshow(image_copy)\n",
    "    if heatmaps is not None:\n",
    "        plt.imshow(np.clip(heatmaps.max(axis=0),0,1)*255,alpha=0.5)\n",
    "    plt.axis(False)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='raw')\n",
    "    plt.close()\n",
    "    buf.seek(0)\n",
    "    save_fig = np.reshape(np.frombuffer(buf.getvalue(), dtype=np.uint8),newshape=(int(fig.bbox.bounds[3]), int(fig.bbox.bounds[2]), -1))\n",
    "    buf.close()\n",
    "    \n",
    "    source_path = pathlib.Path(source_path)\n",
    "    save_path = pathlib.Path('GolferPrediction',source_path.parent.parent.name)\n",
    "    save_path.mkdir(parents=True,exist_ok=True)\n",
    "    plt.imsave(save_path.joinpath(source_path.name),save_fig)\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for batch_test_data in test_loader:\n",
    "    input_images,source_image_paths = batch_test_data\n",
    "    source_images = [plt.imread(f) for f in source_image_paths]\n",
    "    heatmaps, joints_coor, joints_score = predict(model,input_images)\n",
    "    joints_coor = coor_transform(joints_coor, heatmaps, source_images)\n",
    "    heatmaps = heatmaps_resize(heatmaps, source_images)\n",
    "    for i,p in enumerate(source_image_paths):\n",
    "        save_visualize_result(p,source_images[i],joints_coor[i],heatmaps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
